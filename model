import torch
import math
import pywt
import numpy as np
from torch import nn
import torch.nn.functional as F
import logging
import matplotlib.pyplot as plt
import os
from pathlib import Path
from mmcv.cnn import ConvModule, build_norm_layer
from mmengine.model import BaseModule
from mmengine.runner import load_checkpoint
from mmseg.registry import MODELS
from torchvision.models import convnext_tiny, mobilenet_v2
from torchvision.models.feature_extraction import create_feature_extractor

def _make_divisible(v, divisor, min_value=None):
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v

def drop_path(x, drop_prob: float = 0., training: bool = False):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output

class DropPath(nn.Module):
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

def get_shape(tensor):
    shape = tensor.shape
    if torch.onnx.is_in_onnx_export():
        shape = [i.cpu().numpy() for i in shape]
    return shape

class Conv2d_BN(nn.Sequential):
    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,
                 groups=1, bn_weight_init=1,
                 norm_cfg=dict(type='BN', requires_grad=True)):
        super().__init__()
        self.inp_channel = a
        self.out_channel = b
        self.ks = ks
        self.pad = pad
        self.stride = stride
        self.dilation = dilation
        self.groups = groups

        self.add_module('c', nn.Conv2d(
            a, b, ks, stride, pad, dilation, groups, bias=False))
        bn = build_norm_layer(norm_cfg, b)[1]
        nn.init.constant_(bn.weight, bn_weight_init)
        nn.init.constant_(bn.bias, 0)
        self.add_module('bn', bn)

class HSwish(nn.Module):
    def __init__(self, inplace=True):
        super(HSwish, self).__init__()
        self.inplace = inplace

    def forward(self, x):
        return x * torch.nn.functional.relu6(x + 3., inplace=self.inplace) / 6.

class SEModule(nn.Module):
    def __init__(self, channel, reduction=4):
        super(SEModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class InvertedResidual(nn.Module):
    def __init__(
            self,
            inp: int,
            oup: int,
            ks: int,
            stride: int,
            expand_ratio: int,
            use_se=True,
            activations=None,
            norm_cfg=dict(type='BN', requires_grad=True)
    ) -> None:
        super(InvertedResidual, self).__init__()
        self.stride = stride
        self.expand_ratio = expand_ratio
        assert stride in [1, 2]

        if activations is None:
            # activations = nn.ReLU6
            activations = HSwish

        hidden_dim = int(round(inp * expand_ratio))
        self.use_res_connect = self.stride == 1 and inp == oup

        layers = []
        if expand_ratio != 1:
            # pw
            layers.append(Conv2d_BN(inp, hidden_dim, ks=1, norm_cfg=norm_cfg))
            layers.append(activations())
        layers.extend([
            # dw
            Conv2d_BN(hidden_dim, hidden_dim, ks=ks, stride=stride, pad=ks // 2, groups=hidden_dim, norm_cfg=norm_cfg),
            activations(),
            # pw-linear
            Conv2d_BN(hidden_dim, oup, ks=1, norm_cfg=norm_cfg)
        ])

        if use_se:
            layers.append(SEModule(hidden_dim))

        self.conv = nn.Sequential(*layers)
        self.out_channels = oup
        self._is_cn = stride > 1

    def forward(self, x):
        if self.use_res_connect:
            return x + self.conv(x)
        else:
            return self.conv(x)

class GhostModule(nn.Module):
    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True, norm_cfg=dict(type='BN', requires_grad=True)):
        super(GhostModule, self).__init__()
        self.oup = oup
        init_channels = math.ceil(oup / ratio)
        new_channels = init_channels * (ratio - 1)

        self.primary_conv = nn.Sequential(
            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),
            build_norm_layer(norm_cfg, init_channels)[1],
            nn.ReLU(inplace=True) if relu else nn.Sequential(),
        )

        self.cheap_operation = nn.Sequential(
            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),
            build_norm_layer(norm_cfg, new_channels)[1],
            nn.ReLU(inplace=True) if relu else nn.Sequential(),
        )

    def forward(self, x):
        x1 = self.primary_conv(x)
        x2 = self.cheap_operation(x1)
        out = torch.cat([x1, x2], dim=1)
        return out[:, :self.oup, :, :]

class LightFeaturePyramidModule(nn.Module):
    def __init__(
            self,
            cfgs,
            out_indices,
            inp_channel=16,
            activation=HSwish,
            norm_cfg=dict(type='BN', requires_grad=True),
            width_mult=1.,
            use_ghost=True):
        super().__init__()
        self.out_indices = out_indices
        self.use_ghost = use_ghost

        self.stem = nn.Sequential(
            Conv2d_BN(3, inp_channel, 3, 2, 1, norm_cfg=norm_cfg),
            activation()
        )
        self.cfgs = cfgs

        self.layers = []
        self.output_channels = []

        for i, (k, t, c, s) in enumerate(cfgs):
            output_channel = _make_divisible(c * width_mult, 8)
            self.output_channels.append(output_channel)
            exp_size = t * inp_channel
            exp_size = _make_divisible(exp_size * width_mult, 8)
            layer_name = 'layer{}'.format(i + 1)

            if self.use_ghost:
                layer = GhostBottleneck(inp_channel, output_channel, ks=k, stride=s,
                                        expand_ratio=t, norm_cfg=norm_cfg)
            else:
                layer = InvertedResidual(inp_channel, output_channel, ks=k, stride=s,
                                                 expand_ratio=t, norm_cfg=norm_cfg,
                                                 activations=activation)

            self.add_module(layer_name, layer)
            inp_channel = output_channel
            self.layers.append(layer_name)

    def forward(self, x):
        outs = []
        x = self.stem(x)
        for i, layer_name in enumerate(self.layers):
            layer = getattr(self, layer_name)
            x = layer(x)
            if i in self.out_indices:
                outs.append(x)
        return outs

class GhostBottleneck(nn.Module):
    def __init__(self, inp, oup, ks, stride, expand_ratio, norm_cfg=dict(type='BN', requires_grad=True)):
        super(GhostBottleneck, self).__init__()
        self.stride = stride
        assert stride in [1, 2]

        hidden_dim = int(round(inp * expand_ratio))
        self.use_res_connect = self.stride == 1 and inp == oup
        self.ghost1 = GhostModule(inp, hidden_dim, kernel_size=1, relu=True, norm_cfg=norm_cfg)

        if stride > 1:
            self.conv_dw = nn.Sequential(
                nn.Conv2d(hidden_dim, hidden_dim, ks, stride, ks // 2, groups=hidden_dim, bias=False),
                build_norm_layer(norm_cfg, hidden_dim)[1],
            )

        self.se = SEModule(hidden_dim, reduction=hidden_dim // 4 if hidden_dim > 16 else 2)
        self.ghost2 = GhostModule(hidden_dim, oup, kernel_size=1, relu=False, norm_cfg=norm_cfg)

        if self.stride > 1 or inp != oup:
            self.shortcut = nn.Sequential(
                nn.Conv2d(inp, inp, ks, stride, ks // 2, groups=inp, bias=False),
                build_norm_layer(norm_cfg, inp)[1],
                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
                build_norm_layer(norm_cfg, oup)[1],
            )

    def forward(self, x):
        residual = x
        x = self.ghost1(x)

        if self.stride > 1:
            x = self.conv_dw(x)

        x = self.se(x)
        x = self.ghost2(x)

        if self.use_res_connect:
            x = x + residual
        elif hasattr(self, 'shortcut'):
            x = x + self.shortcut(residual)

        return x

class PyramidPoolAgg(nn.Module):
    def __init__(self, in_channels_list, out_channels, stride):
        super().__init__()
        self.stride = stride

        self.weights = nn.Parameter(torch.ones(len(in_channels_list)))
        self.softmax = nn.Softmax(dim=0)

        self.lateral_convs = nn.ModuleList()
        for in_channels in in_channels_list:
            self.lateral_convs.append(nn.Conv2d(in_channels, out_channels, 1))

        self.fusion_conv = nn.Conv2d(out_channels, out_channels, 1)
        self.activation = HSwish()

    def forward(self, inputs):
        B, _, H, W = inputs[-1].shape
        H = (H - 1) // self.stride + 1
        W = (W - 1) // self.stride + 1

        normalized_weights = self.softmax(self.weights)

        resized_features = []
        for i, feat in enumerate(inputs):
            feat = self.lateral_convs[i](feat)
            feat = nn.functional.adaptive_avg_pool2d(feat, (H, W))
            resized_features.append(feat * normalized_weights[i])

        out = sum(resized_features)
        out = self.fusion_conv(out)
        out = self.activation(out)

        return out

class DWT(nn.Module):
    def __init__(self, wave_name='db1', mode='zero', decomposition_level=1):
        super(DWT, self).__init__()
        self.wave_name = wave_name
        self.mode = mode
        self.level = decomposition_level

        wavelet = pywt.Wavelet(wave_name)
        band_low = torch.Tensor(wavelet.rec_lo)
        band_high = torch.Tensor(wavelet.rec_hi)

        self.pad_low = len(band_low) // 2 - 1
        self.pad_high = len(band_high) // 2 - 1

        self.register_buffer('lo_col', band_low.unsqueeze(0).unsqueeze(0).unsqueeze(3))
        self.register_buffer('hi_col', band_high.unsqueeze(0).unsqueeze(0).unsqueeze(3))
        self.register_buffer('lo_row', band_low.unsqueeze(0).unsqueeze(0).unsqueeze(2))
        self.register_buffer('hi_row', band_high.unsqueeze(0).unsqueeze(0).unsqueeze(2))

    def forward(self, x):
        B, C, H, W = x.shape
        if H % 2 != 0 or W % 2 != 0:
            pad_h = 0 if H % 2 == 0 else 1
            pad_w = 0 if W % 2 == 0 else 1
            x = F.pad(x, (0, pad_w, 0, pad_h), mode=self.mode)

        results = []
        current_x = x

        for _ in range(self.level):
            B, C, H, W = current_x.shape

            lo_row_filter = self.lo_row.repeat(C, 1, 1, 1)
            hi_row_filter = self.hi_row.repeat(C, 1, 1, 1)

            lo = F.conv2d(current_x, lo_row_filter, stride=(1, 2), padding=(0, self.pad_low), groups=C)
            hi = F.conv2d(current_x, hi_row_filter, stride=(1, 2), padding=(0, self.pad_high), groups=C)

            lo_col_filter = self.lo_col.repeat(C, 1, 1, 1)
            hi_col_filter = self.hi_col.repeat(C, 1, 1, 1)

            ll = F.conv2d(lo, lo_col_filter, stride=(2, 1), padding=(self.pad_low, 0), groups=C)
            lh = F.conv2d(lo, hi_col_filter, stride=(2, 1), padding=(self.pad_high, 0), groups=C)
            hl = F.conv2d(hi, lo_col_filter, stride=(2, 1), padding=(self.pad_low, 0), groups=C)
            hh = F.conv2d(hi, hi_col_filter, stride=(2, 1), padding=(self.pad_high, 0), groups=C)

            results.append((ll, lh, hl, hh))
            current_x = ll

        return results


class DWTFusion(nn.Module):
    def __init__(self, in_channels, wave_name='db1', decomposition_level=1, use_attention=True, use_residual=True):
        super(DWTFusion, self).__init__()
        self.dwt = DWT(wave_name=wave_name, decomposition_level=decomposition_level)
        self.use_attention = use_attention
        self.use_residual = use_residual

        self.conv_lh = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )

        self.conv_hl = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )

        self.conv_hh = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )

        if use_attention:
            self.attention_lh = ChannelAttention(in_channels)
            self.attention_hl = ChannelAttention(in_channels)
            self.attention_hh = ChannelAttention(in_channels)

        self.fusion = nn.Conv2d(in_channels * 4, in_channels, kernel_size=1)
        self.norm = nn.BatchNorm2d(in_channels)
        self.act = nn.ReLU6(inplace=True)

    def forward(self, x):
        dwt_results = self.dwt(x)
        ll, lh, hl, hh = dwt_results[-1]

        lh_processed = self.conv_lh(lh)
        hl_processed = self.conv_hl(hl)
        hh_processed = self.conv_hh(hh)

        if self.use_attention:
            lh_processed = self.attention_lh(lh_processed) * lh_processed
            hl_processed = self.attention_hl(hl_processed) * hl_processed
            hh_processed = self.attention_hh(hh_processed) * hh_processed

        target_size = x.shape[2:]
        current_size = ll.shape[2:]

        if current_size != target_size:
            ll = self.upsample(ll, target_size)
            lh_processed = self.upsample(lh_processed, target_size)
            hl_processed = self.upsample(hl_processed, target_size)
            hh_processed = self.upsample(hh_processed, target_size)

        fused = torch.cat([ll, lh_processed, hl_processed, hh_processed], dim=1)
        output = self.fusion(fused)
        output = self.norm(output)
        output = self.act(output)

        if self.use_residual:
            return output + x
        return output

    def upsample(self, x, size):
        h_ratio = size[0] / x.shape[2]
        if h_ratio < 2:
            return F.interpolate(x, size=size, mode='nearest')

        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)


class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        reduced_channels = max(in_channels // reduction_ratio, 8)
        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, reduced_channels, kernel_size=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(reduced_channels, in_channels, kernel_size=1, bias=False)
        )

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = self.sigmoid(avg_out + max_out)
        return out

class CrossScaleGatedFuse(torch.nn.Module):
    def __init__(self, dim, dim_s, out_dim, key_dim, num_heads,
                 attn_ratio=2, drop_path=0.,
                 act_layer=HSwish,
                 norm_cfg=dict(type='BN', requires_grad=True)):
        super().__init__()
        self.num_heads = num_heads
        self.scale = key_dim ** -0.5
        self.key_dim = key_dim
        self.nh_kd = nh_kd = key_dim * num_heads
        self.d = int(attn_ratio * key_dim)
        self.dh = int(attn_ratio * key_dim) * num_heads
        groups_q = math.gcd(dim_s, nh_kd)
        groups_k = math.gcd(dim, nh_kd)
        groups_v = math.gcd(dim, self.dh)

        self.to_q = Conv2d_BN(dim_s, nh_kd, 1, groups=groups_q, norm_cfg=norm_cfg)
        self.to_k = Conv2d_BN(dim, nh_kd, 1, groups=groups_k, norm_cfg=norm_cfg)
        self.to_v = Conv2d_BN(dim, self.dh, 1, groups=groups_v, norm_cfg=norm_cfg)

        self.pos_encoding = nn.Sequential(
            Conv2d_BN(dim_s, dim_s, 3, pad=1, groups=dim_s, norm_cfg=norm_cfg),
            act_layer()
        )

        self.reattn_input_dim = self.d + self.key_dim
        self.L_reattn = nn.Linear(self.reattn_input_dim, self.d)

        self.proj = nn.Sequential(
            act_layer(),
            Conv2d_BN(self.dh, dim_s, 1, groups=math.gcd(self.dh, dim_s), bn_weight_init=0, norm_cfg=norm_cfg)
        )
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.output_proj = ConvModule(dim_s, out_dim, kernel_size=1, norm_cfg=norm_cfg, act_cfg=None)

    def forward(self, low, high):
        B, C, H, W = get_shape(high)
        _, C_l, H_l, W_l = get_shape(low)

        low_pos = self.pos_encoding(low) + low
        high_pos = self.pos_encoding(high) + high

        q = self.to_q(low_pos).reshape(B, self.num_heads, self.key_dim, H_l * W_l).permute(0, 1, 3, 2) # [B, heads, H_l*W_l, key_dim]
        k = self.to_k(high_pos).reshape(B, self.num_heads, self.key_dim, H * W) # [B, heads, key_dim, H * W]
        v = self.to_v(high).reshape(B, self.num_heads, self.d, H * W).permute(0, 1, 3, 2)  # [B, heads, H*W, d]

        q = q.softmax(dim=-1)  # [B, heads, H_l*W_l, key_dim]
        k = k.softmax(dim=-2)  # [B, heads, key_dim, H * W]
        k_transposed = k.permute(0, 1, 3, 2)  # [B, heads, H*W, key_dim]
        context = torch.einsum('bhni,bhnj->bhij', k_transposed, v)  # [B, heads, key_dim, d]
        out = torch.einsum('bhni,bhij->bhnj', q, context)  # [B, heads, H_l*W_l, d]

        k_reattn = k.permute(0, 1, 3, 2)  # [B, heads, H * W, key_dim] to [B, heads, H_l*W_l, d]
        if H * W != H_l * W_l:
            k_reshaped = k_reattn.reshape(B * self.num_heads, H, W, self.key_dim)
            k_reshaped = k_reshaped.permute(0, 3, 1, 2)

            k_reshaped = F.interpolate(
                k_reshaped,
                size=(H_l, W_l),
                mode='bilinear',
                align_corners=False
            )  # [B*heads, key_dim, H_l, W_l]
            k_reattn = k_reshaped.permute(0, 2, 3, 1).reshape(B, self.num_heads, H_l * W_l, self.key_dim)

        reattn = torch.cat([out, k_reattn], dim=-1)  # [B*heads*H_l*W_l, 2*d]
        L_reattn = self.L_reattn(reattn)  # [B, heads, H_l*W_l, d]
        out_reattn = torch.sigmoid(L_reattn) * L_reattn  # [B, heads, H_l*W_l, d]

        out_reattn = out_reattn.permute(0, 1, 3, 2).reshape(B, self.dh, H_l, W_l)
        low_weight = self.proj(out_reattn)

        upsample_high = F.interpolate(high, size=(H_l, W_l), mode='bilinear', align_corners=False)

        fuse_f = low_weight * low + upsample_high
        fuse_f = self.output_proj(fuse_f)

        return fuse_f

@MODELS.register_module()
class LWNet(BaseModule):
    def __init__(self, cfgs,
                 channels,
                 out_channels,
                 embed_out_indice,
                 decode_out_indices=[1, 2, 3],
                 num_blocks=2,
                 depths=4,
                 key_dim=16,
                 num_heads=8,
                 attn_ratios=2,
                 mlp_ratios=2,
                 c2t_stride=2,
                 drop_path_rate=0.,
                 reduction_ratio=8,
                 expansion_ratio=4,
                 norm_cfg=dict(type='BN', requires_grad=True),
                 act_layer=HSwish,
                 init_cfg=None,
                 injection=True,
                 wavelet_name='haar',
                 decomposition_level=1,
                 use_dwt=True,
                 ):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels
        self.norm_cfg = norm_cfg
        self.injection = injection
        self.embed_dim = sum(channels)
        self.decode_out_indices = decode_out_indices
        self.init_cfg = init_cfg
        self.num_blocks = num_blocks
        self.use_dwt = use_dwt
        self.depths = depths

        if self.init_cfg != None:
            self.pretrained = self.init_cfg['checkpoint']
        else:
            self.pretrained = None

        self.lfpm = LightFeaturePyramidModule(
            cfgs=cfgs,
            out_indices=embed_out_indice,
            norm_cfg=norm_cfg,
            use_ghost=True
        )

        self.ppa = PyramidPoolAgg(
            in_channels_list=channels,
            out_channels=self.embed_dim,
            stride=c2t_stride
        )

        if self.use_dwt:
            self.dwt_modules = nn.ModuleList([
                DWTFusion(channels[i], wave_name=wavelet_name, decomposition_level=max(1, decomposition_level - i))
                for i in range(depths)
            ])

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depths)]
        self.fuse_trans = nn.ModuleList()
        for i in range(depths):
            if i in self.decode_out_indices:
                self.fuse_trans.append(CrossScaleGatedFuse(
                    self.channels[i], self.channels[i], out_dim=self.out_channels[i], key_dim=key_dim,
                    num_heads=num_heads, attn_ratio=attn_ratios, drop_path=dpr[i] if isinstance(dpr, list) else dpr,
                    norm_cfg=norm_cfg, act_layer=act_layer))
            else:
                self.fuse_trans.append(None)

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                n //= m.groups
                m.weight.data.normal_(0, math.sqrt(2. / n))
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                if m.bias is not None:
                    m.bias.data.zero_()

        if isinstance(self.pretrained, str):
            logger = logging.getLogger()
            checkpoint = load_checkpoint(self.pretrained, logger=logger, map_location='cpu')
            if 'state_dict_ema' in checkpoint:
                state_dict = checkpoint['state_dict_ema']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            elif 'model' in checkpoint:
                state_dict = checkpoint['model']
            else:
                state_dict = checkpoint
            self.load_state_dict(state_dict, False)

    def forward(self, x):
        outputs = self.lfpm(x)

        if self.use_dwt:
            for i in range(self.depths):
                outputs[i] = self.dwt_modules[i](outputs[i])
        out = self.ppa(outputs)

        results = []
        semantics = out.split(self.channels, dim=1)
        for i in range(len(self.channels)):
            if i in self.decode_out_indices:
                out_ = self.fuse_trans[i](outputs[i], semantics[i])
                results.append(out_)

        return results



